{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "C:\\Users\\bbaah\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import matplotlib.animation as animation\n",
    "import re\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, ZeroPadding2D, Conv2D, BatchNormalization, Activation, MaxPooling2D, AveragePooling2D, add, concatenate, Dropout\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model \n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import  ReduceLROnPlateau\n",
    "import warnings\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly as py\n",
    "init_notebook_mode(connected=True)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR_TRAIN = \"/kaggle/input/osic-pulmonary-fibrosis-progression/train/\"\n",
    "IMG_DIR_TEST = \"/kaggle/input/osic-pulmonary-fibrosis-progression/test/\"\n",
    "FILE_DIR = \"/kaggle/input/osic-pulmonary-fibrosis-progression/\"\n",
    "IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /kaggle/input/osic-pulmonary-fibrosis-progression/train.csv does not exist: '/kaggle/input/osic-pulmonary-fibrosis-progression/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-63f13121477e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFILE_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFILE_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File /kaggle/input/osic-pulmonary-fibrosis-progression/train.csv does not exist: '/kaggle/input/osic-pulmonary-fibrosis-progression/train.csv'"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(FILE_DIR + \"train.csv\")\n",
    "test_data = pd.read_csv(FILE_DIR + \"test.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore first patient in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENT_ID = train_data[\"Patient\"][0]    \n",
    "print(\"Patient : \", PATIENT_ID)\n",
    "print(\"Number of FVC observations : \", len(train_data[train_data[\"Patient\"] == PATIENT_ID]))\n",
    "print(\"Age : \", (train_data[train_data[\"Patient\"] == PATIENT_ID][\"Age\"].values[0]))\n",
    "print(\"Sex : \", (train_data[train_data[\"Patient\"] == PATIENT_ID][\"Sex\"].values[0]))\n",
    "print(\"SmokingStatus : \", (train_data[train_data[\"Patient\"] == PATIENT_ID][\"SmokingStatus\"].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Animate the 3-D slice of lungs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "img_names = []\n",
    "for dirictory,_,img in os.walk(IMG_DIR_TRAIN + train_data[\"Patient\"][0]):\n",
    "    img_names.append(img)\n",
    "\n",
    "img_names = natural_sort(img_names[0])\n",
    "\n",
    "images = []\n",
    "k = 0\n",
    "for i in img_names:\n",
    "    images.append([plt.imshow(pydicom.dcmread(IMG_DIR_TRAIN + PATIENT_ID + \"/\" + i).pixel_array, cmap=plt.cm.bone)])\n",
    "    k += 1\n",
    "    \n",
    "ani = animation.ArtistAnimation(fig, images)\n",
    "plt.close()\n",
    "\n",
    "HTML('<center>' + ani.to_html5_video() + '</center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot how FVC changes over time after CT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVC = train_data[train_data[\"Patient\"] == PATIENT_ID][\"FVC\"]\n",
    "Week = train_data[train_data[\"Patient\"] == PATIENT_ID][\"Weeks\"]\n",
    "\n",
    "fig = px.line(x=Week, y=FVC, title='FVC over time of patient with id ' + PATIENT_ID)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title = \"Week\"),\n",
    "    yaxis=dict(title = \"FVC\"),\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=0,\n",
    "            y0=min(FVC),\n",
    "            x1=0,\n",
    "            y1=max(FVC),\n",
    "            line=dict(\n",
    "                color=\"Red\",\n",
    "                width=2,\n",
    "                dash=\"dashdot\",\n",
    "            ),\n",
    "    )\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore all patients in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patients: \", len(train_data[\"Patient\"].unique()))\n",
    "l1 = list(train_data[\"SmokingStatus\"].unique())\n",
    "smokers = \"\"\n",
    "for i in l1:\n",
    "    smokers = smokers + i + \", \"\n",
    "print(\"Among them: \", smokers[:-2])\n",
    "min_Age = min(train_data[\"Age\"].unique())\n",
    "max_Age = max(train_data[\"Age\"].unique())\n",
    "print(\"Ages vary from \", min_Age,\" to \",max_Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"dummy\"] = 1\n",
    "fig = px.bar(train_data.drop_duplicates(subset=[\"Patient\"])[[\"Sex\",\"Age\",\"dummy\"]].groupby([\"Sex\",\"Age\"]).sum().reset_index().rename(columns={\"dummy\":\"Count\"}), x=\"Age\", y=\"Count\",color = \"Sex\")\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smokers distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(train_data.drop_duplicates(subset=[\"Patient\"])[[\"Sex\",\"SmokingStatus\",\"dummy\"]].groupby([\"Sex\",\"SmokingStatus\"]).sum().reset_index().rename(columns={\"dummy\":\"Count\"}), x=\"SmokingStatus\", y=\"Count\",color = \"Sex\")\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 15 patients FVC time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "Pat_Ids = train_data[\"Patient\"].unique()\n",
    "for i in Pat_Ids[:15]:\n",
    "    fig.add_trace(go.Scatter(\n",
    "            x=train_data[train_data[\"Patient\"]==i][\"Weeks\"],\n",
    "            y=train_data[train_data[\"Patient\"]==i][\"FVC\"],\n",
    "            name = i\n",
    "        ))\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=0,\n",
    "            y0=1000,\n",
    "            x1=0,\n",
    "            y1=5000,\n",
    "            line=dict(\n",
    "                color=\"Red\",\n",
    "                width=2,\n",
    "                dash=\"dashdot\",\n",
    "            ),\n",
    "    )\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from graphics above FVC is decreasing with the number of weeks, which may push us to use linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plot to find any correlations in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(train_data, dimensions=[\"Weeks\", \"FVC\", \"Percent\", \"Age\"], color=\"Sex\")\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As our target variable is FVC, we won't use it in PCA or any other technique to reduce dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how many images are available for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files = []\n",
    "for i in Pat_Ids:\n",
    "    num_files.append(len([name for name in os.listdir(IMG_DIR_TRAIN + i + '/') if os.path.isfile(os.path.join(IMG_DIR_TRAIN + i + '/', name))]))\n",
    "fig = go.Figure(go.Bar(name='SF Zoo',x=Pat_Ids,y=num_files))\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns =[\"dummy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here we conclude, that some Patients contain a lot of photos in directory, and we should prepare the data before feeding it in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First let's define what model we will use\n",
    "For this proect I decided to use a model on image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![2020-08-17-15-21-38.png](https://i.postimg.cc/mkNWJwWK/2020-08-17-15-21-38.png)](https://postimg.cc/30RqD2RZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end we will get the lower bound, our result and the upper bound (15%, 50%, 85% quantiles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize and resize our data while uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenCT(Sequence):\n",
    "    \n",
    "    def __init__(self, patients, dataset, cols, batch_size=32, train = 1):\n",
    "        \n",
    "        self.patients = [i for i in patients if i not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.cols = cols\n",
    "        self.patient_scans = {}\n",
    "        self.train = train\n",
    "        IMG_DIR_TRAIN = \"/kaggle/input/osic-pulmonary-fibrosis-progression/train/\"\n",
    "        IMG_DIR_TEST = \"/kaggle/input/osic-pulmonary-fibrosis-progression/test/\"\n",
    "        if train:\n",
    "            self.IMG_DIR = IMG_DIR_TRAIN\n",
    "        else:\n",
    "            self.IMG_DIR = IMG_DIR_TEST\n",
    "        \n",
    "        for patient in patients:\n",
    "            self.patient_scans[patient] = natural_sort([i for i in os.listdir(self.IMG_DIR + patient + \"/\")])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1100\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        CT_Scan = []\n",
    "        Answer, Table = [], [] \n",
    "        \n",
    "        keys = np.random.choice(self.patients, size = self.batch_size)\n",
    "        for key in keys:\n",
    "            try:\n",
    "                idx = np.random.choice(self.patient_scans[key], size=1)[0]\n",
    "                dataset_copy = self.dataset[self.dataset[\"Patient\"] == key]\n",
    "                rand_week = random.choice(list(dataset_copy[\"Weeks\"]))\n",
    "\n",
    "                img = pydicom.dcmread(self.IMG_DIR + key + \"/\" + idx).pixel_array\n",
    "                img_min = img.min()\n",
    "                img_max = img.max()\n",
    "                img = cv2.resize((img - img_min) / (img_max - img_min), (IMAGE_SIZE, IMAGE_SIZE))\n",
    "                CT_Scan.append(img)\n",
    "                Answer.append(dataset_copy[dataset_copy[\"Weeks\"] == rand_week][\"FVC\"].values[0])\n",
    "                Table.append(dataset_copy[dataset_copy[\"Weeks\"] == rand_week][self.cols].values[0])\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        CT_Scan = np.expand_dims(np.array(CT_Scan), axis=-1)\n",
    "        return [CT_Scan, np.array(Table)] , np.array(Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode variables in .csv data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sex - Label encoder;\n",
    "* Smoking Status - Label encoder;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_sex = LabelEncoder()\n",
    "le_smoke = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_sex = le_sex.fit(train_data[\"Sex\"])\n",
    "train_data[\"Sex\"] = le_sex.transform(train_data[\"Sex\"])\n",
    "le_smoke = le_smoke.fit(train_data[\"SmokingStatus\"])\n",
    "train_data[\"SmokingStatus\"] = le_smoke.transform(train_data[\"SmokingStatus\"])\n",
    "test_data[\"Sex\"] = le_sex.transform(test_data[\"Sex\"])\n",
    "test_data[\"SmokingStatus\"] = le_smoke.transform(test_data[\"SmokingStatus\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing it, let's look at distributions of variables we want to normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_distplot([train_data[\"Weeks\"].values], ['Weeks distribution'], show_rug=False)\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_distplot([train_data[\"Percent\"].values], ['Percent distribution'], show_rug=False)\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_distplot([train_data[\"Age\"].values], ['Age distribution'], show_rug=False)\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, that for the first 2 distributions it is better to use RobustScaler, and for the third StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_weeks = RobustScaler().fit(np.array(train_data[\"Weeks\"]).reshape(-1, 1))\n",
    "train_data[\"Weeks\"] = transformer_weeks.transform(np.array(train_data[\"Weeks\"]).reshape(-1, 1)).reshape(1,-1)[0]\n",
    "transformer_perc = RobustScaler().fit(np.array(train_data[\"Percent\"]).reshape(-1, 1))\n",
    "train_data[\"Percent\"] = transformer_perc.transform(np.array(train_data[\"Percent\"]).reshape(-1, 1)).reshape(1,-1)[0]\n",
    "transformer_age = RobustScaler().fit(np.array(train_data[\"Age\"]).reshape(-1, 1))\n",
    "train_data[\"Age\"] = transformer_age.transform(np.array(train_data[\"Age\"]).reshape(-1, 1)).reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"Weeks\"] = transformer_weeks.transform(np.array(test_data[\"Weeks\"]).reshape(-1, 1)).reshape(1,-1)[0]\n",
    "test_data[\"Percent\"] = transformer_perc.transform(np.array(test_data[\"Percent\"]).reshape(-1, 1)).reshape(1,-1)[0]\n",
    "test_data[\"Age\"] = transformer_age.transform(np.array(test_data[\"Age\"]).reshape(-1, 1)).reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now .csv and image data is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First define loss and metric according to the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = tf.constant(70, dtype='float32')\n",
    "C2 = tf.constant(1000, dtype='float32')\n",
    "quantiles = [.15, .50, .85]\n",
    "\n",
    "def metric(y_true, y_pred, Sigma):\n",
    "    Sigma_clipped = np.clip(Sigma, 70, 9e9)  \n",
    "    Delta = np.clip(np.abs(y_true - y_pred), 0 , 1000)  \n",
    "    return np.mean(-1 * (np.sqrt(2) * Delta / Sigma_clipped) - np.log(np.sqrt(2) * Sigma_clipped))\n",
    "\n",
    "def FVC_score(y_true, y_pred):\n",
    "    y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "    y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    sigma = y_pred[:, 2] - y_pred[:, 1]\n",
    "    fvc_pred = y_pred[:, 1]\n",
    "    Sigma_clipped = tf.maximum(sigma, C1)\n",
    "    Delta = tf.abs(y_true[:, 0] - fvc_pred)\n",
    "    Delta = tf.minimum(Delta, C2)\n",
    "    sq2 = tf.sqrt(tf.dtypes.cast(2, dtype=tf.float32))\n",
    "    metric = sq2 * (Delta / Sigma_clipped) * sq2 + tf.math.log(Sigma_clipped * sq2)\n",
    "    return K.mean(metric)\n",
    "\n",
    "def Quantile_loss(y_true, y_pred):\n",
    "    q = tf.constant(np.array([quantiles]), dtype=tf.float32)\n",
    "    y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "    y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q * e, (q - 1) * e)\n",
    "    return K.mean(v)\n",
    "\n",
    "def model_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        lambd = 0.8\n",
    "        return lambd * Quantile_loss(y_true, y_pred) + (1 - lambd) * FVC_score(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a net architecutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, filters):\n",
    "  \n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1))(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, filters):\n",
    "   \n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), strides=(2, 2))(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=(2, 2))(input_tensor)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start1 = Input(shape=(5,),name = \"Tab_input\")\n",
    "start2 = Input(shape=(IMAGE_SIZE, IMAGE_SIZE,1), name = \"Image_input\")\n",
    "\n",
    "x = ZeroPadding2D((3, 3))(start2)\n",
    "x = Conv2D(64, (7, 7), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "x = conv_block(x, [64, 64, 256])\n",
    "x = identity_block(x, [64, 64, 256])\n",
    "x = identity_block(x, [64, 64, 256])\n",
    "\n",
    "x = conv_block(x, [128, 128, 512])\n",
    "x = identity_block(x, [128, 128, 512])\n",
    "x = identity_block(x, [128, 128, 512])\n",
    "x = identity_block(x, [128, 128, 512])\n",
    "\n",
    "x = AveragePooling2D((7, 7))(x)\n",
    "x = Flatten()(x)\n",
    "x1 = Dense(100, activation=\"relu\")(start1)\n",
    "x1 = Dense(100, activation=\"relu\")(x1)\n",
    "x = concatenate([x, x1])\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "\n",
    "out = Dense(3, activation='relu',)(x)\n",
    "model = Model([start2, start1], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=model_loss(), \n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, \n",
    "                                                 epsilon=None, decay=0.01, amsgrad=False), \n",
    "              metrics=[FVC_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add lr decreaser for better perfomance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lr_decr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor= 0.9,\n",
    "    patience=3,\n",
    "    min_lr=1e-5,\n",
    "    mode='min',\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make our Test and Train Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_generator = DataGenCT(patients=Pat_Ids[150:len(Pat_Ids)] ,\n",
    "                            dataset = train_data,\n",
    "                            cols= [\"Weeks\",\"Percent\",\"Age\",\"Sex\",\"SmokingStatus\"],\n",
    "                          )\n",
    "\n",
    "\n",
    "Train_generator = DataGenCT(patients=Pat_Ids[0:150],\n",
    "                            dataset = train_data,\n",
    "                            cols= [\"Weeks\",\"Percent\",\"Age\",\"Sex\",\"SmokingStatus\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(Train_generator , \n",
    "                    steps_per_epoch = 100,\n",
    "                    epochs = 10,\n",
    "                    validation_data = Test_generator,\n",
    "                    use_multiprocessing = False,\n",
    "                    workers = 1,\n",
    "                    callbacks = [Lr_decr],\n",
    "                    validation_steps = 20,\n",
    "                    verbose=1\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "        x=np.r_[1:11],\n",
    "        y=history.history[\"loss\"],\n",
    "        name = \"training loss\"\n",
    "    ))\n",
    "fig.add_trace(go.Scatter(\n",
    "        x=np.r_[1:11],\n",
    "        y=history.history[\"val_loss\"],\n",
    "        name = \"validation loss\"\n",
    "    ))\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title = \"Epoch\"),\n",
    "    yaxis=dict(title = \"Loss\"),\n",
    "    plot_bgcolor='white',\n",
    "    title = \"Loss over epoch\"\n",
    ")\n",
    "\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
